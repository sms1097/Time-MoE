{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Time-MOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/ubuntu/Time-MoE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-19 02:36:36.931148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766111796.950068   27019 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766111796.956187   27019 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766111796.974564   27019 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766111796.974581   27019 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766111796.974583   27019 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766111796.974585   27019 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time_moe.datasets.time_moe_dataset import TimeMoEDataset\n",
    "from time_moe.models.configuration_time_moe import TimeMoeConfig\n",
    "from time_moe.models.modeling_time_moe import TimeMoeForPrediction\n",
    "from time_moe.datasets.time_moe_window_dataset import TimeMoEWindowDataset\n",
    "from time_moe.runner import TimeMoeRunner\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "# torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path(\"./model\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "myparams = dict(\n",
    "    input_size=1,\n",
    "    hidden_size=384,\n",
    "    intermediate_size=1536,\n",
    "    horizon_lengths=1,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    num_key_value_heads=None,\n",
    "    hidden_act=\"silu\",\n",
    "    num_experts_per_tok=2,\n",
    "    num_experts=8,\n",
    "    max_position_embeddings=32768,\n",
    "    initializer_range=0.02,\n",
    "    rms_norm_eps=1e-6,\n",
    "    use_cache=True,\n",
    "    use_dense=False,\n",
    "    rope_theta=10000,\n",
    "    attention_dropout=0.0,\n",
    "    apply_aux_loss=True,\n",
    "    router_aux_loss_factor=0.02,\n",
    "    tie_word_embeddings=False,\n",
    "    patch=False,\n",
    "    patch_len=32,\n",
    "    patch_stride=32,\n",
    ")\n",
    "\n",
    "config = TimeMoeConfig(**myparams)\n",
    "config.save_pretrained(MODEL_DIR)\n",
    "\n",
    "train_config = dict(\n",
    "    from_scratch=True,\n",
    "    train_steps=1,\n",
    "    global_batch_size=8,\n",
    "    micro_batch_size=2,          \n",
    "    normalization_method=\"zero\",\n",
    "    max_length=4096,\n",
    "    stride=None,\n",
    "    data_path=\"Time-300B\",\n",
    "    precision=\"fp32\",\n",
    "    logging_steps=1,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"none\",          # or wandb\n",
    "    torch_compile=False,       # recommended off when profiling\n",
    "    output_dir=\"logs/time_moe\",\n",
    ")\n",
    "\n",
    "# =====================\n",
    "# Runner\n",
    "# =====================\n",
    "\n",
    "runner = TimeMoeRunner(\n",
    "    model_path=str(MODEL_DIR),\n",
    "    output_path=\"logs/time_moe\",\n",
    "    seed=9899,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-19 02:36:53,241 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Set global_batch_size to 8\n",
      "2025-12-19 02:36:53,241 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Set micro_batch_size to 2\n",
      "2025-12-19 02:36:53,242 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Set gradient_accumulation_steps to 4\n",
      "2025-12-19 02:36:53,242 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Set precision to fp32\n",
      "2025-12-19 02:36:53,243 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Set normalization to zero\n",
      "2025-12-19 02:36:53,398 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Use Eager Attention\n",
      "2025-12-19 02:36:55,127 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Load model parameters from: model\n",
      "2025-12-19 02:36:55,129 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: {'train_steps': 1, 'global_batch_size': 8, 'micro_batch_size': 2, 'normalization_method': 'zero', 'max_length': 4096, 'stride': None, 'data_path': 'Time-300B', 'precision': 'fp32', 'logging_steps': 1, 'dataloader_num_workers': 4, 'report_to': 'none', 'torch_compile': False, 'output_dir': 'logs/time_moe'}\n",
      "2025-12-19 02:36:55,131 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: TimeMoETrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=9899,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=False,\n",
      "logging_dir=logs/time_moe/tb_logs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=1,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=1,\n",
      "metric_for_best_model=None,\n",
      "min_learning_rate=0.0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=-1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=logs/time_moe,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=2,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=logs/time_moe,\n",
      "save_on_each_node=False,\n",
      "save_only_model=True,\n",
      "save_safetensors=True,\n",
      "save_steps=None,\n",
      "save_strategy=no,\n",
      "save_total_limit=None,\n",
      "seed=9899,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "2025-12-19 02:36:55,132 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: TimeMoeConfig {\n",
      "  \"apply_aux_loss\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 384,\n",
      "  \"horizon_lengths\": [\n",
      "    1\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_size\": 1,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"time_moe\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_experts\": 8,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 12,\n",
      "  \"patch\": false,\n",
      "  \"patch_len\": 32,\n",
      "  \"patch_stride\": 32,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"router_aux_loss_factor\": 0.02,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_dense\": false\n",
      "}\n",
      "\n",
      "2025-12-19 02:36:55,132 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Number of the model parameters: 113.312M\n",
      "2025-12-19 02:36:55,132 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Tokens will consume: 32.768K\n",
      "2025-12-19 02:36:55,133 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Loading dataset...\n",
      "2025-12-19 02:36:55,135 - log_util.py[pid:27019;line:52:log_in_local_rank_0] - INFO: Processing dataset to fixed-size sub-sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1814/1814 [00:00<00:00, 300718.05it/s]\n"
     ]
    }
   ],
   "source": [
    "runner.train_model(**train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
